# SoluciÃ³n Optimizada: IF + Basic LLM para DivisiÃ³n de Mensajes

## Concepto

**Flujo eficiente que solo usa Basic LLM cuando es necesario:**

1. âœ… Validar longitud del mensaje (IF)
2. âœ… Si â‰¤130 caracteres â†’ enviar directo (sin costo LLM)
3. âœ… Si >130 caracteres â†’ usar Basic LLM para dividir en 2 partes
4. âœ… Enviar cada parte con delay de 5 segundos

---

## Arquitectura por Agente

```
Agente (Explore/Learn/Apply/Review/Optimize)
  â†“
IF (Â¿mensaje > 130 caracteres?)
  â”œâ”€ NO (â‰¤130) â†’ Send WhatsApp â†’ Call Actualizar_memoria
  â”‚
  â””â”€ SÃ (>130) â†’ Basic LLM Chain (divide en 2)
                    â†“
                 Code (parse JSON)
                    â†“
                 Split Out (convierte a items)
                    â†“
                 Loop Over Items (procesa cada parte)
                    â†“
                 Code (add index)
                    â†“
                 IF (Â¿es la primera parte?)
                    â”œâ”€ SÃ â†’ Send WhatsApp
                    â””â”€ NO â†’ Wait 5 Seconds â†’ Send WhatsApp
                              â†“
                   Call Actualizar_memoria
```

---

## Ventajas de esta SoluciÃ³n

âœ… **EconÃ³mica**: Solo usa LLM cuando el mensaje es >130 caracteres
âœ… **RÃ¡pida**: Mensajes cortos se envÃ­an inmediatamente sin procesamiento
âœ… **Inteligente**: DivisiÃ³n semÃ¡ntica cuando es necesario
âœ… **Limpia**: Siempre divide en 2 partes (no 3+)
âœ… **Profesional**: Mejor UX para el usuario final

---

## ImplementaciÃ³n Paso a Paso

### Para cada agente (ejemplo con "Learn")

---

### 1ï¸âƒ£ Agregar Nodo "IF" (Validar Longitud)

**DespuÃ©s del agente "Learn":**

1. Hacer clic en "+" despuÃ©s de "Learn"
2. Buscar "IF"
3. Nombrar: "Message Length Check (Learn)"

**ConfiguraciÃ³n del IF:**

- **Conditions:**
  - **Condition 1:**
    - **Value 1:** `{{ $('Learn').item.json.output.length }}`
    - **Operation:** `larger`
    - **Value 2:** `130`

**Outputs:**
- **True** (mensaje largo) â†’ Ir a "Basic LLM Chain"
- **False** (mensaje corto) â†’ Ir a "Send WhatsApp Message (Short)"

---

### 2ï¸âƒ£ Branch FALSE: EnvÃ­o Directo (â‰¤130 caracteres)

**Nodo "Send WhatsApp Message (Short)"**

Tipo: WhatsApp

**ConfiguraciÃ³n:**
- **Operation:** Send
- **Phone Number ID:** `730111076863264`
- **Recipient Phone Number:** `{{ $('Extraer datos del user').item.json.phone_number }}`
- **Text Body:** `{{ $('Learn').item.json.output }}`

**Conectar a:** `Call 'Actualizar_memoria_utq_bot'1`

---

### 3ï¸âƒ£ Branch TRUE: DivisiÃ³n con LLM (>130 caracteres)

#### A. Nodo "Basic LLM Chain" (Split in 2)

1. Hacer clic en el output TRUE del IF
2. Buscar "Basic LLM Chain"
3. Nombrar: "Split in 2 Parts (Learn)"

**ConfiguraciÃ³n:**

**Prompt:** (Copiar de `prompt_llm_split_2_parts.txt` y adaptar)

```
You are a message splitter for WhatsApp. Your ONLY task is to split a long message into exactly 2 coherent parts.

STRICT RULES:
1. Split the message into EXACTLY 2 parts
2. Each part must be MAXIMUM 130 characters
3. Split at a natural point (end of sentence, paragraph, or logical break)
4. Maintain context and readability in each part
5. Both parts together must contain the complete original message
6. Return ONLY a JSON array with exactly 2 strings

INPUT MESSAGE (more than 130 characters):
{{ $('Learn').item.json.output }}

OUTPUT FORMAT (exactly like this, no other text):
["First part of the message here", "Second part of the message here"]

IMPORTANT:
- Return ONLY the JSON array
- EXACTLY 2 strings in the array, no more, no less
- No explanations, no markdown, no code blocks
- Each string must be â‰¤130 characters
- Find the best natural split point around the middle of the message
```

**Model:** Conectar a "OpenAI Chat Model" o "OpenAI Chat Model2"

---

#### B. Nodo "Code" (Parse JSON)

**Nombre:** `Parse JSON (Learn)`

**CÃ³digo:**

```javascript
// Obtener la respuesta del LLM
const llmOutput = $input.item.json.output || $input.item.json.text || '';

// Parsear el JSON
let messageParts = [];
try {
  // Limpiar y parsear
  const cleaned = llmOutput.trim().replace(/```json\n?/g, '').replace(/```\n?/g, '');
  messageParts = JSON.parse(cleaned);

  // Validar que sea un array con 2 elementos
  if (!Array.isArray(messageParts) || messageParts.length !== 2) {
    throw new Error('Expected exactly 2 parts');
  }
} catch (error) {
  // Si falla el parsing, dividir manualmente en 2 partes
  console.error('Error parsing JSON:', error);
  const original = $('Learn').item.json.output;
  const midPoint = Math.floor(original.length / 2);

  // Buscar espacio mÃ¡s cercano al punto medio
  let cutPoint = original.lastIndexOf(' ', midPoint);
  if (cutPoint === -1) cutPoint = midPoint;

  messageParts = [
    original.substring(0, cutPoint).trim(),
    original.substring(cutPoint).trim()
  ];
}

// Obtener datos adicionales
const phoneNumber = $('Extraer datos del user').item.json.phone_number;

return [{
  json: {
    message_parts: messageParts,
    phone_number: phoneNumber,
    total_parts: 2
  }
}];
```

---

#### C. Nodo "Split Out" (Convert to Items)

**Nombre:** `Split Into Items (Learn)`

**ConfiguraciÃ³n:**
- **Field to Split Out:** `message_parts`
- **Include Other Fields:** âœ… Yes
- **Destination Field Name:** `message_part`

Este nodo convierte el array `[part1, part2]` en 2 items separados.

---

#### D. Nodo "Loop Over Items"

**Nombre:** `Loop Messages (Learn)`

**ConfiguraciÃ³n:**
- Mode: Run Once for Each Item

Este nodo procesa cada parte secuencialmente (importante para el delay).

---

#### E. Nodo "Code" (Add Index)

**Dentro del Loop, nombrar:** `Add Index (Learn)`

**CÃ³digo:**

```javascript
// Obtener el Ã­ndice del loop
const loopNode = $input.context.getNodeParameter('Loop Messages (Learn)');
const currentIndex = $input.itemIndex;

return [{
  json: {
    ...$input.item.json,
    part_number: currentIndex + 1,
    is_first_message: currentIndex === 0
  }
}];
```

---

#### F. Nodo "IF" (Is First Message?)

**Nombre:** `Is First Message? (Learn)`

**ConfiguraciÃ³n:**
- **Condition:** `{{ $json.is_first_message }}` equals `true`

**Outputs:**
- **True** â†’ Send WhatsApp Message (Long)
- **False** â†’ Wait 5 Seconds

---

#### G. Nodo "Wait" (Solo para segunda parte)

**Nombre:** `Wait 5 Seconds (Learn)`

**ConfiguraciÃ³n:**
- **Wait Amount:** 5
- **Wait Unit:** Seconds

**Conectar a:** Send WhatsApp Message (Long)

---

#### H. Nodo "Send WhatsApp Message (Long)"

**Nombre:** `Send WhatsApp Message (Long)`

**ConfiguraciÃ³n:**
- **Operation:** Send
- **Phone Number ID:** `730111076863264`
- **Recipient Phone Number:** `{{ $json.phone_number }}`
- **Text Body:** `{{ $json.message_part }}`

**Conectar a:** `Call 'Actualizar_memoria_utq_bot'1`

---

## Diagrama Visual Completo

```
Learn
  â†“
Message Length Check (Learn) [IF length > 130?]
  â”‚
  â”œâ”€ FALSE (â‰¤130) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚                                       â”‚
  â”‚  Send WhatsApp Message (Short)       â”‚
  â”‚    â†“                                  â”‚
  â”‚                                       â”‚
  â””â”€ TRUE (>130)                          â”‚
       â†“                                  â”‚
     Split in 2 Parts (Learn) [LLM]      â”‚
       â†“                                  â”‚
     Parse JSON (Learn)                   â”‚
       â†“                                  â”‚
     Split Into Items (Learn)             â”‚
       â†“                                  â”‚
     Loop Messages (Learn)                â”‚
       â†“                                  â”‚
     Add Index (Learn)                    â”‚
       â†“                                  â”‚
     Is First Message? (Learn) [IF]       â”‚
       â”œâ”€ TRUE â†’ Send WhatsApp (Long) â”€â”€â”€â”¤
       â””â”€ FALSE â†’ Wait 5s â†’ Send (Long) â”€â”¤
                                          â†“
                          Call 'Actualizar_memoria_utq_bot'1
```

---

## Casos de Uso

### Caso 1: Mensaje Corto (â‰¤130 caracteres)

**Input:** "Hola, Â¿cÃ³mo estÃ¡s hoy?"

**Flujo:**
```
Learn â†’ IF (25 chars â‰¤130) â†’ FALSE â†’ Send WhatsApp (Short) â†’ Call Actualizar
```

**Resultado:**
- âœ… 1 mensaje enviado
- âœ… Sin delay
- âœ… Sin costo de LLM
- âš¡ Latencia mÃ­nima

---

### Caso 2: Mensaje Largo (>130 caracteres)

**Input:** "Hola, te cuento que el Business Model Canvas es una herramienta estratÃ©gica que permite visualizar y diseÃ±ar modelos de negocio de forma clara. Se compone de 9 bloques fundamentales que cubren las Ã¡reas clave." (199 caracteres)

**Flujo:**
```
Learn â†’ IF (199 chars >130) â†’ TRUE â†’ LLM â†’ Parse â†’ Split â†’ Loop
  â†’ Parte 1: Send directo
  â†’ Parte 2: Wait 5s â†’ Send
  â†’ Call Actualizar
```

**LLM Output:**
```json
[
  "Hola, te cuento que el Business Model Canvas es una herramienta estratÃ©gica que permite visualizar y diseÃ±ar modelos de negocio.",
  "Se compone de 9 bloques fundamentales que cubren las Ã¡reas clave."
]
```

**Resultado:**
- âœ… 2 mensajes enviados
- âœ… 5 segundos de delay entre ellos
- âœ… DivisiÃ³n inteligente (frases completas)
- ðŸ’° Costo: ~$0.00001-$0.0001

---

## Replicar para los 5 Agentes

Repetir la estructura completa para cada agente:

| Agente | Nodo Actualizar Memoria |
|--------|-------------------------|
| **Explore** | Call 'Actualizar_memoria_utq_bot' |
| **Learn** | Call 'Actualizar_memoria_utq_bot'1 |
| **Apply** | Call 'Actualizar_memoria_utq_bot'2 |
| **Review** | Call 'Actualizar_memoria_utq_bot'3 |
| **Optimize** | Call 'Actualizar_memoria_utq_bot'4 |

**Importante:** En cada Basic LLM, adaptar:
```
{{ $('Learn').item.json.output }} â†’ {{ $('Explore').item.json.output }}
{{ $('Learn').item.json.output }} â†’ {{ $('Apply').item.json.output }}
... etc
```

---

## Optimizaciones Adicionales

### 1. Cache del LLM (Reducir Costos)

Si el mismo mensaje largo se repite, considera agregar cache:

```javascript
// En el nodo Parse JSON, agregar antes del try:
const cacheKey = `split_${hash($('Learn').item.json.output)}`;
const cached = $getWorkflowStaticData(cacheKey);

if (cached) {
  return [{ json: cached }];
}

// ... resto del cÃ³digo ...

// DespuÃ©s del parsing exitoso:
$setWorkflowStaticData(cacheKey, result);
```

### 2. Modelo EconÃ³mico para DivisiÃ³n

Usar GPT-3.5-turbo en lugar de GPT-4 solo para la divisiÃ³n:

- **GPT-4:** ~$0.0001 por divisiÃ³n
- **GPT-3.5-turbo:** ~$0.00001 por divisiÃ³n (10x mÃ¡s barato)

Crear un "OpenAI Chat Model" adicional solo para divisiÃ³n con modelo `gpt-3.5-turbo`.

---

## MÃ©tricas y Monitoreo

### Agregar Nodo de Logging (Opcional)

Antes de "Call Actualizar_memoria", agregar nodo Code:

```javascript
const messageLength = $('Learn').item.json.output.length;
const usedLLM = messageLength > 130;
const parts = usedLLM ? 2 : 1;

console.log({
  agent: 'Learn',
  message_length: messageLength,
  used_llm: usedLLM,
  parts_sent: parts,
  timestamp: new Date().toISOString()
});

return [$input.item];
```

Esto te permite ver en los logs:
- CuÃ¡ntos mensajes requirieron LLM
- Longitudes promedio
- Costos estimados

---

## Testing Completo

### Test 1: Mensaje Muy Corto
```
Input: "Hola"
Esperado: 1 mensaje, sin delay, sin LLM
```

### Test 2: Mensaje Justo en el LÃ­mite
```
Input: "x" * 130
Esperado: 1 mensaje, sin delay, sin LLM
```

### Test 3: Mensaje Ligeramente Largo
```
Input: "x" * 131
Esperado: 2 mensajes, 5s delay, usa LLM
```

### Test 4: Mensaje Muy Largo
```
Input: "x" * 250
Esperado: 2 mensajes, 5s delay, usa LLM
```

### Test 5: Mensaje con Saltos de LÃ­nea
```
Input: "PÃ¡rrafo 1\n\nPÃ¡rrafo 2\n\nPÃ¡rrafo 3..." (>130)
Esperado: DivisiÃ³n inteligente en pÃ¡rrafos
```

---

## ComparaciÃ³n de Soluciones

| CaracterÃ­stica | SoluciÃ³n Optimizada (IF + LLM) | Solo CÃ³digo | Solo LLM |
|----------------|-------------------------------|-------------|----------|
| **Costo** | Bajo (solo cuando >130) | $0 | Alto (siempre) |
| **Velocidad (â‰¤130)** | âš¡ Inmediata | âš¡ Inmediata | ðŸŒ +1-2s |
| **Velocidad (>130)** | +1-2s (LLM) | âš¡ Inmediata | +1-2s (LLM) |
| **Calidad DivisiÃ³n** | â­â­â­â­â­ SemÃ¡ntica | â­â­â­ MecÃ¡nica | â­â­â­â­â­ SemÃ¡ntica |
| **Eficiencia** | â­â­â­â­â­ Ã“ptima | â­â­â­â­ Buena | â­â­ Baja |
| **Complejidad n8n** | Media (7-8 nodos) | Baja (1 nodo) | Media (5-6 nodos) |

**ðŸ† Ganador: SoluciÃ³n Optimizada IF + LLM**

---

## Costos Estimados

### Escenario: 1000 mensajes/mes

Suponiendo que **30% de mensajes >130 caracteres:**

**SoluciÃ³n Optimizada:**
- 700 mensajes cortos: $0 (sin LLM)
- 300 mensajes largos: 300 Ã— $0.00001 = **$0.003/mes**
- Total: **~$0.003/mes** (insignificante)

**Solo LLM (sin IF):**
- 1000 mensajes: 1000 Ã— $0.00001 = **$0.01/mes**

**Ahorro: ~70%** ðŸ’°

---

## SoluciÃ³n de Problemas

### Error: "Expected exactly 2 parts"
**Causa:** El LLM no retornÃ³ exactamente 2 partes
**SoluciÃ³n:** El cÃ³digo tiene fallback que divide manualmente

### Delay no funciona entre partes
**Causa:** Loop no estÃ¡ en modo secuencial
**SoluciÃ³n:** Verificar Loop Over Items estÃ¡ en "Run Once for Each Item"

### Primera parte tiene delay
**Causa:** LÃ³gica del IF "Is First Message?" invertida
**SoluciÃ³n:** Verificar que `is_first_message === true` va directo a Send

### Mensaje no se divide
**Causa:** IF de longitud mal configurado
**SoluciÃ³n:** Verificar `{{ $('Learn').item.json.output.length }} > 130`

---

## Resumen de Nodos por Agente

### Branch Corto (â‰¤130):
1. IF (Message Length Check)
2. Send WhatsApp Message (Short)
3. Call Actualizar_memoria

### Branch Largo (>130):
1. IF (Message Length Check)
2. Basic LLM Chain (Split in 2)
3. Code (Parse JSON)
4. Split Out
5. Loop Over Items
6. Code (Add Index)
7. IF (Is First Message?)
8. Wait 5 Seconds (solo para segunda parte)
9. Send WhatsApp Message (Long)
10. Call Actualizar_memoria

**Total: ~10 nodos por agente** (pero solo se usan 3 si el mensaje es corto)

---

## Archivos de Referencia

1. `prompt_llm_split_2_parts.txt` - Prompt optimizado para dividir en 2
2. `SOLUCION_OPTIMIZADA_IF_LLM.md` - Esta guÃ­a completa

---

## ConclusiÃ³n

Esta soluciÃ³n optimizada ofrece **lo mejor de ambos mundos:**

âœ… **Eficiencia**: Solo procesa con LLM cuando es necesario
âœ… **Velocidad**: Mensajes cortos son instantÃ¡neos
âœ… **Calidad**: DivisiÃ³n inteligente cuando se requiere
âœ… **EconomÃ­a**: Ahorra ~70% en costos vs siempre usar LLM
âœ… **UX**: Mejor experiencia de usuario

**RecomendaciÃ³n: Implementar esta soluciÃ³n en los 5 agentes** ðŸš€
